#!/usr/bin/env python3
"""
LLMåŠ¨æ€åˆ†ææ¨¡å—ç¯å¢ƒæ£€æŸ¥è„šæœ¬
æ£€æŸ¥Pythonç‰ˆæœ¬ã€å†…å­˜ã€OpenAI APIé…ç½®å’Œè¿æ¥çŠ¶æ€
"""

import asyncio
import os
import sys
from typing import List, Tuple

import psutil


def check_python_version() -> Tuple[bool, str]:
    """æ£€æŸ¥Pythonç‰ˆæœ¬ï¼ˆè¦æ±‚>=3.11ï¼‰"""
    version = sys.version_info
    if version.major == 3 and version.minor >= 11:
        return (
            True,
            f"âœ… Python {version.major}.{version.minor}.{version.micro}",
        )
    else:
        return (
            False,
            f"âŒ Python {version.major}.{version.minor}.{version.micro} (éœ€è¦ >= 3.11)",
        )


def check_memory() -> Tuple[bool, str]:
    """æ£€æŸ¥ç³»ç»Ÿå†…å­˜ï¼ˆæœ€å°‘2GB/å»ºè®®4GBï¼‰"""
    try:
        memory = psutil.virtual_memory()
        total_gb = memory.total / (1024**3)

        if total_gb >= 4:
            return True, f"âœ… ç³»ç»Ÿå†…å­˜: {total_gb:.1f}GB (å……è¶³)"
        elif total_gb >= 2:
            return True, f"âš ï¸  ç³»ç»Ÿå†…å­˜: {total_gb:.1f}GB (åŸºæœ¬æ»¡è¶³ï¼Œå»ºè®®4GB)"
        else:
            return False, f"âŒ ç³»ç»Ÿå†…å­˜: {total_gb:.1f}GB (ä¸è¶³ï¼Œè‡³å°‘éœ€è¦2GB)"
    except Exception as e:
        return False, f"âŒ æ— æ³•æ£€æµ‹å†…å­˜: {str(e)}"


def check_api_key() -> Tuple[bool, str]:
    """æ£€æŸ¥OpenAI APIå¯†é’¥é…ç½®"""
    from cli_config import get_llm_api_key, is_llm_configured

    if not is_llm_configured():
        return False, "âŒ OPENAI_API_KEY ç¯å¢ƒå˜é‡æœªè®¾ç½®"

    api_key = get_llm_api_key()

    if not api_key.startswith("sk-"):
        return False, "âŒ APIå¯†é’¥æ ¼å¼é”™è¯¯ (åº”ä»¥ 'sk-' å¼€å¤´)"

    if len(api_key) < 20:
        return False, "âŒ APIå¯†é’¥é•¿åº¦å¼‚å¸¸ (è¿‡çŸ­)"

    # è„±æ•æ˜¾ç¤º
    masked_key = api_key[:10] + "..." + api_key[-4:]
    return True, f"âœ… APIå¯†é’¥å·²é…ç½®: {masked_key}"


def check_env_file() -> Tuple[bool, str]:
    """æ£€æŸ¥.envæ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    env_path = ".env"

    if os.path.exists(env_path):
        return True, f"âœ… ç¯å¢ƒé…ç½®æ–‡ä»¶å­˜åœ¨: {env_path}"
    else:
        return False, f"âŒ ç¯å¢ƒé…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {env_path}"


async def check_api_connection() -> Tuple[bool, str]:
    """æ£€æŸ¥APIè¿æ¥çŠ¶æ€"""
    try:
        from cli_config import is_llm_configured
        from llm_dynamic.analyzer_api import DynamicLLMAnalyzerAPI

        # æ£€æŸ¥APIå¯†é’¥æ˜¯å¦é…ç½®
        if not is_llm_configured():
            return False, "âŒ APIå¯†é’¥æœªé…ç½®ï¼Œæ— æ³•æµ‹è¯•è¿æ¥"

        # åˆ›å»ºåˆ†æå™¨å¹¶æµ‹è¯•è¿æ¥
        analyzer = DynamicLLMAnalyzerAPI()
        result = await analyzer.test_connection()

        if result["status"] == "success":
            return True, f"âœ… APIè¿æ¥æˆåŠŸ: {result['message']}"
        else:
            return False, f"âŒ APIè¿æ¥å¤±è´¥: {result['message']}"

    except ImportError as e:
        return False, f"âŒ å¯¼å…¥æ¨¡å—å¤±è´¥: {str(e)}"
    except Exception as e:
        return False, f"âŒ APIè¿æ¥æµ‹è¯•å¤±è´¥: {str(e)}"


def check_database_exists() -> Tuple[bool, str]:
    """æ£€æŸ¥æ•°æ®åº“æ–‡ä»¶æ˜¯å¦å­˜åœ¨"""
    try:
        # ä½¿ç”¨é…ç½®ç®¡ç†è·å–æ•°æ®åº“è·¯å¾„
        from cli_config import get_database_path

        db_path = get_database_path()

        # å¤„ç†ç›¸å¯¹è·¯å¾„
        from pathlib import Path

        path = Path(db_path)
        if not path.is_absolute():
            path = Path(__file__).parent / path

        if path.exists():
            size = path.stat().st_size / 1024  # KB
            return True, f"âœ… æ•°æ®åº“æ–‡ä»¶å­˜åœ¨: {size:.1f}KB ({path})"
        else:
            return False, f"âŒ æ•°æ®åº“æ–‡ä»¶ä¸å­˜åœ¨: {path}"
    except Exception as e:
        return False, f"âŒ æ•°æ®åº“æ£€æŸ¥å¤±è´¥: {str(e)}"


def check_dependencies() -> Tuple[bool, str]:
    """æ£€æŸ¥Pythonä¾èµ–"""
    required_packages = [
        "langchain",
        "langchain_openai",
        "langchain_core",
        "tortoise",
        "models",
    ]
    missing = []

    for package in required_packages:
        try:
            if package == "langchain_openai":
                __import__("langchain_openai")
            elif package == "langchain_core":
                __import__("langchain_core")
            else:
                __import__(package)
        except ImportError:
            missing.append(package)

    if not missing:
        return True, "âœ… æ‰€æœ‰ä¾èµ–å·²å®‰è£…"
    else:
        return False, f"âŒ ç¼ºå°‘ä¾èµ–: {', '.join(missing)}"


def get_model_recommendations() -> List[str]:
    """æ¨èçš„æ¨¡å‹åˆ—è¡¨"""
    return [
        "gpt-3.5-turbo (ç»æµå®ç”¨ï¼Œé€Ÿåº¦å¿«)",
        "gpt-4 (é«˜ç²¾åº¦ï¼Œå¤æ‚åˆ†æ)",
        "gpt-4-turbo (å¹³è¡¡é€‰æ‹©)",
        "gpt-4o (æœ€æ–°æ¨¡å‹)",
        "gpt-4o-mini (è½»é‡çº§é€‰æ‹©)",
    ]


def get_current_model_config() -> str:
    """è·å–å½“å‰æ¨¡å‹é…ç½®"""
    from cli_config import get_llm_model

    return get_llm_model()


def print_setup_guide():
    """æ‰“å°é…ç½®æŒ‡å—"""
    print("\n" + "=" * 60)
    print("ğŸ› ï¸  é…ç½®æŒ‡å—")
    print("=" * 60)

    print("\n1. å¤åˆ¶ç¯å¢ƒé…ç½®æ–‡ä»¶:")
    print("   cp env.example .env")

    print("\n2. è·å–OpenAI APIå¯†é’¥:")
    print("   â€¢ è®¿é—® https://platform.openai.com/api-keys")
    print("   â€¢ æ³¨å†Œ/ç™»å½•OpenAIè´¦æˆ·")
    print("   â€¢ åˆ›å»ºæ–°çš„APIå¯†é’¥")

    print("\n3. é…ç½®ç¯å¢ƒå˜é‡ (.envæ–‡ä»¶):")
    print("   OPENAI_API_KEY=sk-your-api-key-here")
    print("   OPENAI_MODEL=gpt-3.5-turbo")
    print("   # OPENAI_BASE_URL=https://api.openai.com/v1  # å¯é€‰")

    print("\n4. éªŒè¯é…ç½®:")
    print("   python check_llm_env.py")

    print("\n5. æµ‹è¯•åŠŸèƒ½:")
    print("   python llm_cli.py 'æµ‹è¯•åˆ†æåŠŸèƒ½'")

    print("\nğŸ“‹ è¯¦ç»†é…ç½®æŒ‡å—:")
    print("   docs/API_SETUP_GUIDE.md")


async def main():
    """ä¸»æ£€æŸ¥ç¨‹åº"""
    print("ğŸ” LLMåŠ¨æ€åˆ†ææ¨¡å—ç¯å¢ƒæ£€æŸ¥")
    print("=" * 60)

    # åŒæ­¥æ£€æŸ¥é¡¹ç›®
    sync_checks = [
        ("Pythonç‰ˆæœ¬", check_python_version),
        ("ç³»ç»Ÿå†…å­˜", check_memory),
        ("ç¯å¢ƒé…ç½®æ–‡ä»¶", check_env_file),
        ("APIå¯†é’¥é…ç½®", check_api_key),
        ("æ•°æ®åº“æ–‡ä»¶", check_database_exists),
        ("Pythonä¾èµ–", check_dependencies),
    ]

    all_passed = True

    for name, check_func in sync_checks:
        success, message = check_func()
        print(f"{name:12} | {message}")
        if not success:
            all_passed = False

    # å¼‚æ­¥æ£€æŸ¥APIè¿æ¥
    print(f"{'APIè¿æ¥æµ‹è¯•':12} | ", end="", flush=True)
    success, message = await check_api_connection()
    print(message)
    if not success:
        all_passed = False

    print("\n" + "-" * 60)

    # æ˜¾ç¤ºå½“å‰é…ç½®
    print("ğŸ“‹ å½“å‰é…ç½®:")
    from cli_config import get_llm_base_url

    model = get_current_model_config()
    print(f"   ğŸ¤– æ¨¡å‹: {model}")

    base_url = get_llm_base_url()
    print(f"   ğŸŒ APIç«¯ç‚¹: {base_url}")

    print("\n" + "-" * 60)

    if all_passed:
        print("ğŸ‰ ç¯å¢ƒæ£€æŸ¥é€šè¿‡ï¼å¯ä»¥å¼€å§‹ä½¿ç”¨LLMåŠ¨æ€åˆ†æåŠŸèƒ½")
        print("\nå¿«é€Ÿå¼€å§‹:")
        print("   python llm_cli.py 'æ‰¾å‡ºæ€§ä»·æ¯”æœ€é«˜çš„å•†å“'")
        print("   python llm_cli.py 'åˆ†æiPhoneä»·æ ¼è¶‹åŠ¿' iPhone")
        print("   python llm_cli.py --interactive  # äº¤äº’æ¨¡å¼")
    else:
        print("âŒ ç¯å¢ƒæ£€æŸ¥æœªé€šè¿‡ï¼Œè¯·å‚è€ƒé…ç½®æŒ‡å—")
        print_setup_guide()

    print("\n" + "=" * 60)
    print("ğŸ’¡ æ¨èæ¨¡å‹:")
    for model in get_model_recommendations():
        print(f"   â€¢ {model}")

    print("\nğŸ’° æˆæœ¬æç¤º:")
    print("   â€¢ gpt-3.5-turbo: çº¦ $0.001/1K tokens")
    print("   â€¢ gpt-4: çº¦ $0.03/1K tokens")
    print("   â€¢ å»ºè®®ä» gpt-3.5-turbo å¼€å§‹ä½¿ç”¨")


if __name__ == "__main__":
    asyncio.run(main())
