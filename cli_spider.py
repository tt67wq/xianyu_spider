#!/usr/bin/env python3
"""
é—²é±¼å•†å“æœç´¢CLIå·¥å…·
å°†FastAPIç‰ˆæœ¬çš„çˆ¬è™«åŠŸèƒ½æ”¹é€ ä¸ºå‘½ä»¤è¡Œå·¥å…·
"""

import argparse
import asyncio
import csv
import hashlib
import json
import sys
import time
from datetime import datetime
from typing import Any, List

from playwright.async_api import async_playwright

from cli_config import (
    CLIError,
    DatabaseError,
    OutputError,
    SpiderError,
    get_config,
    handle_cli_error,
)
from database import close_database, get_database_info, init_database
from models import XianyuProduct
from utils.price_parser import parse_price_to_cents

# è·å–é…ç½®å®ä¾‹
config = get_config()


def get_md5(text: str) -> str:
    """è¿”å›ç»™å®šæ–‡æœ¬çš„MD5å“ˆå¸Œå€¼"""
    return hashlib.md5(text.encode("utf-8")).hexdigest()


def get_link_unique_key(link: str) -> str:
    """
    æˆªå–é“¾æ¥ä¸­å‰1ä¸ª"&"ä¹‹å‰çš„å†…å®¹ä½œä¸ºå”¯ä¸€æ ‡è¯†ä¾æ®ã€‚
    å¦‚æœé“¾æ¥ä¸­çš„"&"å°‘äº1ä¸ªï¼Œåˆ™è¿”å›æ•´ä¸ªé“¾æ¥ã€‚
    """
    parts = link.split("&", 1)
    if len(parts) >= 2:
        return "&".join(parts[:1])
    else:
        return link


async def safe_get(data: Any, *keys: Any, default: Any = "æš‚æ— ") -> Any:
    """å®‰å…¨è·å–åµŒå¥—å­—å…¸å€¼"""
    for key in keys:
        try:
            data = data[key]
        except (KeyError, TypeError, IndexError):
            return default
    return data


async def save_to_db(
    data_list: List[dict], verbose: bool = False
) -> tuple[int, List[int]]:
    """
    é€æ¡ä¿å­˜æ•°æ®åˆ°æ•°æ®åº“ï¼Œè‹¥ç›¸åŒé“¾æ¥ï¼ˆæŒ‰æˆªå–è§„åˆ™åˆ¤æ–­ï¼‰çš„è®°å½•å·²å­˜åœ¨åˆ™è·³è¿‡ï¼Œ
    åŒæ—¶ç»Ÿè®¡å½“å‰å…³é”®è¯ä¸‹æ–°å¢çš„è®°å½•æ•°é‡ï¼Œå¹¶è¿”å›æ–°å¢è®°å½•çš„ id åˆ—è¡¨
    """
    new_records = 0
    new_ids = []

    for i, item in enumerate(data_list, 1):
        try:
            link = item["å•†å“é“¾æ¥"]
            # å…ˆæˆªå–é“¾æ¥å†…å®¹
            unique_part = get_link_unique_key(link)
            # è®¡ç®—å”¯ä¸€æ ‡è¯†çš„ MD5 å“ˆå¸Œå€¼
            link_hash = get_md5(unique_part)

            product, created = await XianyuProduct.get_or_create(
                link_hash=link_hash,
                defaults={
                    "title": item["å•†å“æ ‡é¢˜"],
                    "price": item["å½“å‰å”®ä»·"],
                    "price_cents": item.get("ä»·æ ¼åˆ†", -1),
                    "area": item["å‘è´§åœ°åŒº"],
                    "seller": item["å–å®¶æ˜µç§°"],
                    "link": link,
                    "image_url": item["å•†å“å›¾ç‰‡é“¾æ¥"],
                    "publish_time": datetime.strptime(
                        item["å‘å¸ƒæ—¶é—´"], "%Y-%m-%d %H:%M"
                    )
                    if item["å‘å¸ƒæ—¶é—´"] != "æœªçŸ¥æ—¶é—´"
                    else None,
                },
            )

            if created:
                new_records += 1
                new_ids.append(product.id)
                if verbose:
                    print(
                        f"âœ… æ–°å¢è®°å½• {i}/{len(data_list)}: {item['å•†å“æ ‡é¢˜'][:30]}..."
                    )
            else:
                if verbose:
                    print(
                        f"â­ï¸  è·³è¿‡é‡å¤ {i}/{len(data_list)}: {item['å•†å“æ ‡é¢˜'][:30]}..."
                    )

        except Exception as e:
            print(f"âŒ ä¿å­˜æ•°æ®å‡ºé”™ {i}/{len(data_list)}: {str(e)}")

    return new_records, new_ids


async def scrape_xianyu(
    keyword: str, max_pages: int = 1, verbose: bool = False, quiet: bool = False
) -> List[dict]:
    """å¼‚æ­¥çˆ¬å–é—²é±¼å•†å“æ•°æ®"""
    data_list = []
    spider_config = config.get_spider_config()

    if not quiet:
        print(f"ğŸš€ å¼€å§‹æœç´¢: '{keyword}' (æœ€å¤š {max_pages} é¡µ)")

    async with async_playwright() as p:
        browser = await p.chromium.launch(
            headless=spider_config["browser_headless"]
        )
        context = await browser.new_context(
            user_agent=spider_config["user_agent"]
        )
        page = await context.new_page()

        async def on_response(response):
            """å¤„ç†APIå“åº”ï¼Œè§£ææ•°æ®"""
            if (
                "h5api.m.goofish.com/h5/mtop.taobao.idlemtopsearch.pc.search"
                in response.url
            ):
                try:
                    result_json = await response.json()
                    items = result_json.get("data", {}).get("resultList", [])

                    for item in items:
                        main_data = await safe_get(
                            item,
                            "data",
                            "item",
                            "main",
                            "exContent",
                            default={},
                        )
                        click_params = await safe_get(
                            item,
                            "data",
                            "item",
                            "main",
                            "clickParam",
                            "args",
                            default={},
                        )

                        # è§£æå•†å“ä¿¡æ¯
                        title = await safe_get(
                            main_data, "title", default="æœªçŸ¥æ ‡é¢˜"
                        )

                        # ä»·æ ¼å¤„ç†
                        price_parts = await safe_get(
                            main_data, "price", default=[]
                        )
                        price = "ä»·æ ¼å¼‚å¸¸"
                        if isinstance(price_parts, list):
                            price = "".join(
                                [
                                    str(p.get("text", ""))
                                    for p in price_parts
                                    if isinstance(p, dict)
                                ]
                            )
                            price = price.replace("å½“å‰ä»·", "").strip()
                            if "ä¸‡" in price:
                                price = f"Â¥{float(price.replace('Â¥', '').replace('ä¸‡', '')) * 10000:.0f}"

                        # è§£æä»·æ ¼ä¸ºæ•´æ•°(åˆ†)
                        price_cents = parse_price_to_cents(price)

                        # å…¶ä»–å­—æ®µè§£æ
                        area = await safe_get(
                            main_data, "area", default="åœ°åŒºæœªçŸ¥"
                        )
                        seller = await safe_get(
                            main_data, "userNickName", default="åŒ¿åå–å®¶"
                        )
                        raw_link = await safe_get(
                            item,
                            "data",
                            "item",
                            "main",
                            "targetUrl",
                            default="",
                        )
                        image_url = await safe_get(
                            main_data, "picUrl", default=""
                        )

                        data_list.append(
                            {
                                "å•†å“æ ‡é¢˜": title,
                                "å½“å‰å”®ä»·": price,
                                "ä»·æ ¼åˆ†": price_cents,
                                "å‘è´§åœ°åŒº": area,
                                "å–å®¶æ˜µç§°": seller,
                                "å•†å“é“¾æ¥": raw_link.replace(
                                    "fleamarket://", "https://www.goofish.com/"
                                ),
                                "å•†å“å›¾ç‰‡é“¾æ¥": f"https:{image_url}"
                                if image_url
                                and not image_url.startswith("http")
                                else image_url,
                                "å‘å¸ƒæ—¶é—´": datetime.fromtimestamp(
                                    int(click_params.get("publishTime", 0))
                                    / 1000
                                ).strftime("%Y-%m-%d %H:%M")
                                if isinstance(
                                    click_params.get("publishTime"), str
                                )
                                and click_params.get(
                                    "publishTime", ""
                                ).isdigit()
                                else "æœªçŸ¥æ—¶é—´",
                            }
                        )

                except Exception as e:
                    if verbose:
                        print(f"âŒ å“åº”å¤„ç†å¼‚å¸¸: {str(e)}")

        try:
            # è®¿é—®é¦–é¡µå¹¶æ“ä½œé¡µé¢
            if verbose:
                print("ğŸŒ æ­£åœ¨è®¿é—®é—²é±¼é¦–é¡µ...")
            await page.goto("https://www.goofish.com")
            await page.fill('input[class*="search-input"]', keyword)
            await page.click('button[type="submit"]')

            # å¦‚æœå­˜åœ¨å¼¹çª—å¹¿å‘Šåˆ™å…³é—­
            try:
                await page.wait_for_selector(
                    "div[class*='closeIconBg']", timeout=5000
                )
                await page.click("div[class*='closeIconBg']")
                if verbose:
                    print("âœ… å…³é—­å¹¿å‘Šå¼¹çª—")
            except Exception:
                if verbose:
                    print("â„¹ï¸  æœªå‘ç°å¹¿å‘Šå¼¹çª—")
                pass

            await page.click("text=æ–°å‘å¸ƒ")
            await page.click("text=æœ€æ–°")

            # æ³¨å†Œå“åº”ç›‘å¬
            page.on("response", on_response)

            # åˆ†é¡µå¤„ç†
            current_page = 1
            while current_page <= max_pages:
                if not quiet:
                    print(f"ğŸ“„ æ­£åœ¨å¤„ç†ç¬¬ {current_page} é¡µ...")
                await asyncio.sleep(
                    spider_config["request_delay"]
                )  # ç­‰å¾…æ•°æ®åŠ è½½

                if current_page < max_pages:
                    # æŸ¥æ‰¾ä¸‹ä¸€é¡µæŒ‰é’®
                    next_btn = await page.query_selector(
                        "[class*='search-pagination-arrow-right']:not([disabled])"
                    )
                    if not next_btn:
                        if verbose:
                            print("â„¹ï¸  å·²åˆ°è¾¾æœ€åä¸€é¡µ")
                        break
                    await next_btn.click()

                current_page += 1

        finally:
            await browser.close()

    return data_list


def print_results_table(data_list: List[dict], limit: int = 10):
    """ä»¥è¡¨æ ¼å½¢å¼æ‰“å°ç»“æœ"""
    if not data_list:
        print("ğŸ“ æ²¡æœ‰æ‰¾åˆ°å•†å“æ•°æ®")
        return

    ui_config = config.get_ui_config()
    title_len = ui_config["title_max_length"]
    price_len = ui_config["price_max_length"]
    area_len = ui_config["area_max_length"]
    seller_len = ui_config["seller_max_length"]

    print(f"\nğŸ“Š æ‰¾åˆ° {len(data_list)} ä¸ªå•†å“:")
    print("=" * 120)
    print(
        f"{'åºå·':<4} {'æ ‡é¢˜':<{title_len}} {'ä»·æ ¼':<{price_len}} {'åœ°åŒº':<{area_len}} {'å–å®¶':<{seller_len}}"
    )
    print("=" * 120)

    for i, item in enumerate(data_list[:limit], 1):
        title = (
            item["å•†å“æ ‡é¢˜"][: title_len - 2] + ".."
            if len(item["å•†å“æ ‡é¢˜"]) > title_len
            else item["å•†å“æ ‡é¢˜"]
        )
        price = (
            item["å½“å‰å”®ä»·"][: price_len - 2] + ".."
            if len(item["å½“å‰å”®ä»·"]) > price_len
            else item["å½“å‰å”®ä»·"]
        )
        area = (
            item["å‘è´§åœ°åŒº"][: area_len - 2] + ".."
            if len(item["å‘è´§åœ°åŒº"]) > area_len
            else item["å‘è´§åœ°åŒº"]
        )
        seller = (
            item["å–å®¶æ˜µç§°"][: seller_len - 2] + ".."
            if len(item["å–å®¶æ˜µç§°"]) > seller_len
            else item["å–å®¶æ˜µç§°"]
        )

        print(
            f"{i:<4} {title:<{title_len}} {price:<{price_len}} {area:<{area_len}} {seller:<{seller_len}}"
        )

    if len(data_list) > limit:
        print(f"... è¿˜æœ‰ {len(data_list) - limit} ä¸ªå•†å“æœªæ˜¾ç¤º")
    print("=" * 120)


def save_to_json(data_list: List[dict], filename: str) -> bool:
    """ä¿å­˜ä¸ºJSONæ ¼å¼"""
    try:
        output_config = config.get_output_config()
        with open(filename, "w", encoding="utf-8") as f:
            json.dump(
                data_list,
                f,
                ensure_ascii=False,
                indent=output_config["json_indent"],
            )
        return True
    except Exception as e:
        raise OutputError(f"ä¿å­˜JSONæ–‡ä»¶å¤±è´¥: {e}")


def save_to_csv(data_list: List[dict], filename: str) -> bool:
    """ä¿å­˜ä¸ºCSVæ ¼å¼"""
    try:
        if not data_list:
            raise OutputError("æ²¡æœ‰æ•°æ®å¯ä¿å­˜")

        output_config = config.get_output_config()
        with open(
            filename, "w", newline="", encoding=output_config["csv_encoding"]
        ) as f:
            writer = csv.DictWriter(f, fieldnames=data_list[0].keys())
            writer.writeheader()
            writer.writerows(data_list)
        return True
    except Exception as e:
        raise OutputError(f"ä¿å­˜CSVæ–‡ä»¶å¤±è´¥: {e}")


async def cli_search(args):
    """CLIæœç´¢å‘½ä»¤å¤„ç†"""
    start_time = time.time()

    try:
        # éªŒè¯å‚æ•°
        pages = config.validate_pages(args.pages)
        limit = config.validate_table_limit(args.limit)
        format_name = config.validate_output_format(args.format)

        # åˆå§‹åŒ–æ•°æ®åº“
        if not args.no_db:
            if not await init_database():
                raise DatabaseError("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")

            if args.verbose:
                db_info = await get_database_info()
                print(f"ğŸ“¦ æ•°æ®åº“: {db_info['database_path']}")
                print(f"ğŸ“Š ç°æœ‰è®°å½•: {db_info['record_count']} æ¡")

        # æ‰§è¡Œæœç´¢
        data_list = await scrape_xianyu(
            args.keyword, pages, args.verbose, args.quiet
        )

        if not data_list:
            if not args.quiet:
                print("ğŸ˜” æ²¡æœ‰æ‰¾åˆ°ä»»ä½•å•†å“")
            return 0

        # ä¿å­˜åˆ°æ•°æ®åº“
        new_count = 0
        new_ids = []
        if not args.no_db:
            if not args.quiet:
                print("ğŸ’¾ æ­£åœ¨ä¿å­˜åˆ°æ•°æ®åº“...")
            new_count, new_ids = await save_to_db(data_list, args.verbose)

        # è¾“å‡ºç»“æœ
        elapsed_time = time.time() - start_time

        if not args.quiet:
            print(f"\nğŸ‰ æœç´¢å®Œæˆ! ç”¨æ—¶: {elapsed_time:.2f}ç§’")
            print(f"ğŸ“¦ æ‰¾åˆ°å•†å“: {len(data_list)} ä¸ª")
            if not args.no_db:
                print(f"ğŸ’¾ æ–°å¢è®°å½•: {new_count} æ¡")

        # æ ¹æ®è¾“å‡ºæ ¼å¼æ˜¾ç¤ºç»“æœ
        if format_name == "table":
            print_results_table(data_list, limit)
        elif format_name == "json":
            if args.output:
                save_to_json(data_list, args.output)
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                output_config = config.get_output_config()
                print(
                    json.dumps(
                        data_list,
                        ensure_ascii=False,
                        indent=output_config["json_indent"],
                    )
                )
        elif format_name == "csv":
            if args.output:
                save_to_csv(data_list, args.output)
                print(f"âœ… ç»“æœå·²ä¿å­˜åˆ°: {args.output}")
            else:
                raise OutputError("CSVæ ¼å¼éœ€è¦æŒ‡å®šè¾“å‡ºæ–‡ä»¶ --output")

        return 0

    except (CLIError, ValueError) as e:
        return handle_cli_error(e, args.debug)

    except Exception as e:
        return handle_cli_error(SpiderError(f"æœç´¢å¤±è´¥: {e}"), args.debug)

    finally:
        if not args.no_db:
            await close_database()


async def cli_info(args):
    """æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯"""
    try:
        if not await init_database():
            raise DatabaseError("æ•°æ®åº“åˆå§‹åŒ–å¤±è´¥")

        db_info = await get_database_info()

        print("ğŸ“Š æ•°æ®åº“ä¿¡æ¯:")
        print(f"   è·¯å¾„: {db_info['database_path']}")
        print(f"   æ–‡ä»¶å­˜åœ¨: {'æ˜¯' if db_info['database_exists'] else 'å¦'}")
        print(f"   å·²åˆå§‹åŒ–: {'æ˜¯' if db_info['is_initialized'] else 'å¦'}")
        print(f"   è®°å½•æ€»æ•°: {db_info['record_count']} æ¡")

        if args.verbose:
            config.print_config_summary()

            if db_info["record_count"] > 0:
                # æ˜¾ç¤ºæœ€æ–°å‡ æ¡è®°å½•
                latest_products = (
                    await XianyuProduct.all().order_by("-id").limit(5)
                )
                print(f"\nğŸ“ æœ€æ–° {len(latest_products)} æ¡è®°å½•:")
                for i, product in enumerate(latest_products, 1):
                    print(f"   {i}. {product.title[:40]}... - {product.price}")

        return 0

    except CLIError as e:
        return handle_cli_error(e, args.debug)

    except Exception as e:
        return handle_cli_error(
            DatabaseError(f"è·å–æ•°æ®åº“ä¿¡æ¯å¤±è´¥: {e}"), args.debug
        )

    finally:
        await close_database()


def create_parser():
    """åˆ›å»ºå‘½ä»¤è¡Œå‚æ•°è§£æå™¨"""
    spider_config = config.get_spider_config()
    ui_config = config.get_ui_config()
    output_config = config.get_output_config()

    parser = argparse.ArgumentParser(
        description="é—²é±¼å•†å“æœç´¢CLIå·¥å…·",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹ç”¨æ³•:
  %(prog)s search "iPhone 14"                    # æœç´¢iPhone 14
  %(prog)s search "iPhone 14" -p 3               # æœç´¢3é¡µ
  %(prog)s search "iPhone 14" --format json      # JSONæ ¼å¼è¾“å‡º
  %(prog)s search "iPhone 14" -o results.csv     # ä¿å­˜ä¸ºCSV
  %(prog)s search "iPhone 14" --no-db            # ä¸ä¿å­˜åˆ°æ•°æ®åº“
  %(prog)s info                                  # æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯
  %(prog)s info -v                               # æ˜¾ç¤ºè¯¦ç»†æ•°æ®åº“ä¿¡æ¯
        """,
    )

    # å…¨å±€é€‰é¡¹
    parser.add_argument("-v", "--verbose", action="store_true", help="è¯¦ç»†è¾“å‡º")
    parser.add_argument("-q", "--quiet", action="store_true", help="é™é»˜æ¨¡å¼")
    parser.add_argument("--debug", action="store_true", help="è°ƒè¯•æ¨¡å¼")

    subparsers = parser.add_subparsers(dest="command", help="å¯ç”¨å‘½ä»¤")

    # æœç´¢å‘½ä»¤
    search_parser = subparsers.add_parser("search", help="æœç´¢å•†å“")
    search_parser.add_argument("keyword", help="æœç´¢å…³é”®è¯")
    search_parser.add_argument(
        "-p",
        "--pages",
        type=int,
        default=spider_config["max_pages_default"],
        help=f"æœ€å¤§æœç´¢é¡µæ•° (é»˜è®¤: {spider_config['max_pages_default']}, é™åˆ¶: {spider_config['max_pages_limit']})",
    )
    search_parser.add_argument(
        "--format",
        choices=output_config["supported_formats"],
        default=output_config["default_format"],
        help=f"è¾“å‡ºæ ¼å¼ (é»˜è®¤: {output_config['default_format']})",
    )
    search_parser.add_argument(
        "-o", "--output", help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ (ç”¨äºjson/csvæ ¼å¼)"
    )
    search_parser.add_argument(
        "--no-db", action="store_true", help="ä¸ä¿å­˜åˆ°æ•°æ®åº“"
    )
    search_parser.add_argument(
        "--limit",
        type=int,
        default=ui_config["table_max_rows_default"],
        help=f"è¡¨æ ¼æ˜¾ç¤ºçš„æœ€å¤§è¡Œæ•° (é»˜è®¤: {ui_config['table_max_rows_default']}, é™åˆ¶: {ui_config['table_max_rows_limit']})",
    )

    # ä¿¡æ¯å‘½ä»¤
    subparsers.add_parser("info", help="æ˜¾ç¤ºæ•°æ®åº“ä¿¡æ¯")

    return parser


async def main():
    """ä¸»å‡½æ•°"""
    parser = create_parser()
    args = parser.parse_args()

    # å‚æ•°éªŒè¯
    if (
        hasattr(args, "quiet")
        and hasattr(args, "verbose")
        and args.quiet
        and args.verbose
    ):
        print("âŒ --quiet å’Œ --verbose ä¸èƒ½åŒæ—¶ä½¿ç”¨")
        return 1

    if not args.command:
        parser.print_help()
        return 1

    print("=" * 60)
    print("ğŸ•·ï¸  é—²é±¼å•†å“æœç´¢CLIå·¥å…·")
    print("=" * 60)

    try:
        if args.command == "search":
            return await cli_search(args)
        elif args.command == "info":
            return await cli_info(args)
        else:
            raise CLIError(f"æœªçŸ¥å‘½ä»¤: {args.command}")

    except Exception as e:
        return handle_cli_error(e, getattr(args, "debug", False))


if __name__ == "__main__":
    exit_code = asyncio.run(main())
    sys.exit(exit_code)
